train:
  epochs: 10
  batch: 8

  scheduler: constant
  warmup: 0.01
  lr: 0.001

test:
  batch: 16

model:
  dim: 128
  max_len: 512

  positional:
    dropout: 0.3

  encoder:
    layer_num: 4
    head_num: 2
    fc_dim: 256
    dropout: 0.1

    gate:
      path: "model.gate.nvidia"

  decoder:
    layer_num: 2
    head_num: 2
    fc_dim: 64
    dropout: 0.1

  classifier:
    dropout: 0.3

tokenizer:
  path: "FacebookAI/roberta-base"
  vocab: 50265
  mask: 50264
  pad: 1

dataset:
  path: "Yelp/yelp_review_full"
  text_col: "text"

checkpoint:
  interval: 15
  output: "output"

log:
  k: 100
