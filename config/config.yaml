train:
  epochs: 10
  batch: 32

  scheduler: lin
  warmup: 0.1

test:
  batch: 256

Model:
  max_seq_len: 512
  embed_dim: 512
  batch_first: True


  encoder:
    num_layers: 12
    n_head: 8
    dim_fc: 3072
    dropout: 0.1

  decoder:
    num_layers: 3
    n_head: 8
    dim_fc: 3072
    dropout: 0.1

Tokenizer:
  google-bert/bert-base-uncased:
    pad: 0
    mask: 103
    vocab: 30522

  FacebookAI/roberta-base:
    pad: 1
    mask: 50264
    vocab: 50265